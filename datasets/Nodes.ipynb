{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from algorithms.TranAD.TranAD import TranAD\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "# from logger import logger\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "def average_weights(state_dicts: List[dict], fed_avg_freqs: torch.Tensor):\n",
    "    # init\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = state_dicts[0][key] * fed_avg_freqs[0]\n",
    "\n",
    "    state_dicts = state_dicts[1:]\n",
    "    fed_avg_freqs = fed_avg_freqs[1:]\n",
    "    for state_dict, freq in zip(state_dicts, fed_avg_freqs):\n",
    "        for key in state_dict.keys():\n",
    "            avg_state_dict[key] += state_dict[key] * freq\n",
    "    return avg_state_dict\n",
    "\n",
    "\n",
    "def update_global_grad_correct(old_correct: dict, grad_correct_deltas: List[dict], fed_avg_freqs: torch.Tensor, num_chosen_client, num_total_client):\n",
    "    assert (len(grad_correct_deltas) == num_chosen_client)\n",
    "    total_delta = average_weights(grad_correct_deltas, [1 / num_chosen_client] * num_chosen_client)\n",
    "    for key in old_correct.keys():\n",
    "        if key in total_delta.keys():\n",
    "            old_correct[key] = old_correct[key] + total_delta[key]\n",
    "    return old_correct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "def get_init_grad_correct(model: nn.Module):\n",
    "    correct = {}\n",
    "    for name, _ in model.named_parameters():\n",
    "        correct[name] = torch.tensor(0, dtype=torch.float, device=\"cpu\")\n",
    "    return correct\n",
    "\n",
    "# def load_model(state_dict) -> nn.Module:\n",
    "#     if args.tsadalg != 'deep_svdd':\n",
    "#         model = model_fun()\n",
    "#         model.load_state_dict(state_dict)\n",
    "#         return model\n",
    "#     else:\n",
    "#         if config_svdd[\"stage\"] == \"first\":\n",
    "#             model = Model_first_stage()\n",
    "#         elif config_svdd[\"stage\"] == \"second\":\n",
    "#             model = Model_second_stage()\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "#         model.load_state_dict(state_dict)\n",
    "#         return model\n",
    "\n",
    "class Client(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.net = TranAD(25)\n",
    "        self.dataset = dataset\n",
    "        self.state_dict_prev = None\n",
    "        self.moon_mu = 1\n",
    "        self.prox_mu = 0.01\n",
    "        self.local_bs = 64\n",
    "        self.grad_correct = get_init_grad_correct(TranAD(25))\n",
    "        self.local_ep = 1\n",
    "        self.criterion = nn.MSELoss().to(device)\n",
    "        self.cos_sim = torch.nn.CosineSimilarity(dim=-1).to(device)\n",
    "        self.temperature = 0.5\n",
    "        self.trainloader = None\n",
    "        self.verbose = 1\n",
    "\n",
    "    def set_local_optimizer(self, model: nn.Module) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(model.parameters, lr=0.001)\n",
    "\n",
    "    def local_train(\n",
    "            self, global_state_dict, global_round, global_grad_correct: dict, global_c: torch.Tensor = None\n",
    "    ):\n",
    "\n",
    "        scheduler = None\n",
    "\n",
    "        # region 准备 model model_prev model_global\n",
    "\n",
    "        model_global = self.net.load_state_dict(global_state_dict)\n",
    "        model_global.requires_grad_(False)\n",
    "        model_global.eval()\n",
    "        model_global.to(device)\n",
    "        #\n",
    "        model_current = self.net.load_state_dict(global_state_dict)\n",
    "        model_current.requires_grad_(True)\n",
    "        model_current.train()\n",
    "        model_current.to(device)\n",
    "        # endregion\n",
    "\n",
    "        epoch_loss = []\n",
    "        train_acc = []  # 返回的loss和acc都是local round上的平均\n",
    "\n",
    "        # region Set optimizer and dataloader\n",
    "        optimizer = self.set_local_optimizer(model_current)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, 0.9)\n",
    "\n",
    "        trainloader = DataLoader(\n",
    "                    self.dataset,\n",
    "                    batch_size=self.local_bs,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=1,\n",
    "                    drop_last=False\n",
    "            )\n",
    "\n",
    "        l1s = []\n",
    "        for local_epoch in range(self.local_ep):\n",
    "            loss1_list = []\n",
    "            batch_loss = []\n",
    "            correct = 0\n",
    "            num_data = 0\n",
    "            for i, (x, y) in enumerate(trainloader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                local_bs = x.shape[0]\n",
    "                feats = x.shape[-1]\n",
    "                window = x.permute(1, 0, 2)\n",
    "                elem = window[-1, :, :].view(1, local_bs, feats)\n",
    "                feature, logits, others = model_current(window, elem)\n",
    "                l = nn.MSELoss(reduction='none')\n",
    "                n = local_epoch + 1\n",
    "                z = (others['x1'], others['x2'])\n",
    "                l1 = l(z, elem) if not isinstance(z, tuple) else (1 / n) * l(z[0], elem) + (1 - 1 / n) * l(\n",
    "                        z[1],\n",
    "                        elem\n",
    "                )\n",
    "                if isinstance(z, tuple): z = z[1]\n",
    "                l1s.append(torch.mean(l1).item())\n",
    "                loss = torch.mean(l1)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                state_dict_current = model_current.state_dict()\n",
    "                lr = optimizer.state_dict()['param_groups'][-1]['lr']\n",
    "                for key in state_dict_current:\n",
    "                    # if not state_dict_current[key].requires_grad:\n",
    "                    #     continue\n",
    "                    if key == 'pos_encoder.pe':\n",
    "                        continue\n",
    "                    c_global = global_grad_correct[key].to(device)\n",
    "                    c_local = self.grad_correct[key].to(device)\n",
    "                    state_dict_current[key] -= lr * (c_global - c_local)\n",
    "                model_current.load_state_dict(state_dict_current)\n",
    "\n",
    "            batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "            print(\n",
    "                            f'| Global Round : {global_round} | Local Epoch : {local_epoch} |' +\n",
    "                            f' Training Loss: {loss.item():.6f}\\t'\n",
    "                            # + f' Training Accuracy: {train_acc[-1]:.6f}'\n",
    "                    )\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        mean_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "        model_current.cpu()\n",
    "        self.state_dict_prev = model_current.state_dict()\n",
    "        c_delta_local = None\n",
    "        return float(mean_loss), None, c_delta_local\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "def generate_clients(datasets: List[Dataset]) -> List[Client]:\n",
    "    clients = []\n",
    "    for dataset in datasets:\n",
    "        clients.append(Client(dataset))\n",
    "    return clients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from torch.distributions import Dirichlet\n",
    "\n",
    "current_path = os.getcwd()\n",
    "data_dir = current_path + '/data/datasets/psm/raw'\n",
    "train_path = current_path + '/data/datasets/psm/raw/train'\n",
    "test_path = current_path + '/data/datasets/psm/raw/test'\n",
    "test_labels_path = current_path + '/data/datasets/psm/raw/test_label'\n",
    "\n",
    "num_clients = 10\n",
    "beta = 0.5\n",
    "\n",
    "def generate_data_nums(num_client, num_data, beta=0.5):\n",
    "    while True:\n",
    "        data_num_each_client = Dirichlet(torch.tensor([beta] * num_client)).sample()\n",
    "        data_num_each_client = torch.floor(num_data * data_num_each_client)\n",
    "        data_num_each_client[-1] = num_data - torch.sum(data_num_each_client[:-1])\n",
    "        if not (0 in data_num_each_client):\n",
    "            break\n",
    "    return data_num_each_client\n",
    "\n",
    "class PSM_truncated(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False, window_len=10):\n",
    "\n",
    "        self.root = root\n",
    "        self.dataidxs = dataidxs\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.download = download\n",
    "        self.window_len = window_len\n",
    "\n",
    "        global scalers\n",
    "        self.scalers = scalers\n",
    "\n",
    "        self.data, self.target = self.__build_truncated_dataset__()\n",
    "\n",
    "    def __build_truncated_dataset__(self):\n",
    "\n",
    "        # current_path = os.getcwd()\n",
    "        current_path = 'E:\\\\pythonProject\\\\FedTADBench-main'\n",
    "\n",
    "        if self.train:\n",
    "            train_path = current_path + '\\\\data\\\\datasets\\\\psm\\\\raw'\n",
    "            data = []\n",
    "            this_data = pd.read_csv(train_path + '/train.csv')\n",
    "            this_data.drop(columns=[r'timestamp_(min)'], inplace=True)\n",
    "            data_length = this_data.values.shape[0]\n",
    "            if beta >= 10000:\n",
    "                each_length = data_length // num_clients\n",
    "                this_data_values = this_data.values.astype(np.float32)\n",
    "                this_data_values = np.nan_to_num(this_data_values)\n",
    "                this_data_values = self.scalers[0].fit_transform(this_data_values)\n",
    "                for i in range(num_clients):\n",
    "                    data.append(this_data_values[i * each_length: (i + 1) * each_length])\n",
    "            else:\n",
    "                lengths = generate_data_nums(num_client=num_clients, num_data=data_length, beta=beta)\n",
    "                lengths = lengths.detach().cpu().numpy()\n",
    "                lengths = lengths.astype(int)\n",
    "                lengths = lengths.tolist()\n",
    "                start = 0\n",
    "                this_data_values = this_data.values.astype(np.float32)\n",
    "                this_data_values = np.nan_to_num(this_data_values)\n",
    "                this_data_values = self.scalers[0].fit_transform(this_data_values)\n",
    "                for li in range(len(lengths)):\n",
    "                    l = lengths[li]\n",
    "                    # if start + l <= this_data_values.shape[0] - 1:\n",
    "                    if start + l <= this_data_values.shape[0]:\n",
    "                        data.append(this_data_values[start: start + l])\n",
    "                    else:\n",
    "                        data.append(this_data_values[start:])\n",
    "                    start += l\n",
    "            target = data.copy()\n",
    "        else:\n",
    "            test_path = current_path + '\\\\data\\\\datasets\\\\psm\\\\raw'\n",
    "            this_data = pd.read_csv(test_path + '/test.csv')\n",
    "            this_data.drop(columns=[r'timestamp_(min)'], inplace=True)\n",
    "            data = this_data.values\n",
    "            data = data.astype(np.float32)\n",
    "            data = np.nan_to_num(data)\n",
    "            data = self.scalers[0].transform(data)\n",
    "            data_length = this_data.values.shape[0]\n",
    "            test_target_path = current_path + '\\\\data\\\\datasets\\\\psm\\\\raw\\\\test_label.csv'\n",
    "            # print(test_target_path)\n",
    "            target_csv = pd.read_csv(test_target_path)\n",
    "            target_csv.drop(columns=[r'timestamp_(min)'], inplace=True)\n",
    "            target = target_csv.values\n",
    "            target = target.astype(np.float32)\n",
    "\n",
    "        if self.dataidxs is not None:\n",
    "            data = data[self.dataidxs]\n",
    "            target = target[self.dataidxs]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if index + 1 - self.window_len < 0:\n",
    "            data = self.data[0: index + 1]\n",
    "            delta = self.window_len - data.shape[0]\n",
    "            data0 = np.repeat(data[0][np.newaxis, :], delta, axis=0)\n",
    "            data = np.concatenate((data0, data), axis=0)\n",
    "        else:\n",
    "            data = self.data[index + 1 - self.window_len: index + 1]\n",
    "        target = self.target[index]\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def get_dataset(dataset, datadir, dataidxs=None, noise_level=0):\n",
    "    train_ds = PSM_truncated(datadir, dataidxs=dataidxs, train=True, transform=None, download=True)\n",
    "    test_ds = PSM_truncated(datadir, train=False, transform=None, download=True)\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "def psm_noniid():\n",
    "    train_ds_locals, test_ds_locals = [None] * num_clients, [None] * num_clients\n",
    "    chosen_idxes = [i for i in range(num_clients)]\n",
    "    for i in range(len(chosen_idxes)):\n",
    "        dataidxs = chosen_idxes[i]\n",
    "        train_ds_locals[i], test_ds_locals[i] = get_dataset(\n",
    "            \"psm\", data_dir, dataidxs\n",
    "        )\n",
    "    return train_ds_locals\n",
    "\n",
    "client_datasets_non_iid = psm_noniid()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "2\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'requires_grad_'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [189], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m client_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     28\u001B[0m data_nums\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mlen\u001B[39m(client\u001B[38;5;241m.\u001B[39mdataset))\n\u001B[1;32m---> 29\u001B[0m loss, accuracy, grad_correct_delta \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mglobal_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mglobal_round\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mglobal_correct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m client_times\u001B[38;5;241m.\u001B[39mappend(time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m client_start)\n\u001B[0;32m     35\u001B[0m grad_correct_deltas\u001B[38;5;241m.\u001B[39mappend(grad_correct_delta)\n",
      "Cell \u001B[1;32mIn [186], line 51\u001B[0m, in \u001B[0;36mClient.local_train\u001B[1;34m(self, global_state_dict, global_round, global_grad_correct, global_c)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# region 准备 model model_prev model_global\u001B[39;00m\n\u001B[0;32m     50\u001B[0m model_global \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnet\u001B[38;5;241m.\u001B[39mload_state_dict(global_state_dict)\n\u001B[1;32m---> 51\u001B[0m \u001B[43mmodel_global\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequires_grad_\u001B[49m(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     52\u001B[0m model_global\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     53\u001B[0m model_global\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_IncompatibleKeys' object has no attribute 'requires_grad_'"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "client_rate = 0.2\n",
    "\n",
    "clients = generate_clients(client_datasets_non_iid)\n",
    "\n",
    "model = TranAD(25).cpu()\n",
    "global_state_dict = model.state_dict()\n",
    "global_correct = get_init_grad_correct(TranAD(25).cpu())\n",
    "\n",
    "for global_round in tqdm(range(epochs), file=sys.stdout):\n",
    "    print(f'\\n | Global Training Round : {global_round + 1} |\\n')\n",
    "\n",
    "    num_active_client = int((len(clients) * client_rate))\n",
    "    print(num_active_client)\n",
    "    ind_active_clients = np.random.choice(range(len(clients)), num_active_client, replace=False)\n",
    "    active_clients = [clients[i] for i in ind_active_clients]\n",
    "\n",
    "    # endregion\n",
    "\n",
    "    active_state_dict = []\n",
    "    data_nums = []\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    grad_correct_deltas = []\n",
    "    client_times = []\n",
    "    for client in active_clients:\n",
    "        client_start = time.time()\n",
    "        data_nums.append(len(client.dataset))\n",
    "        loss, accuracy, grad_correct_delta = client.local_train(\n",
    "            global_state_dict,\n",
    "            global_round,\n",
    "            global_correct,\n",
    "            )\n",
    "        client_times.append(time.time() - client_start)\n",
    "        grad_correct_deltas.append(grad_correct_delta)\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        active_state_dict.append(client.state_dict_prev)\n",
    "#\n",
    "#     # end region\n",
    "#\n",
    "#     this_time = max(client_times)\n",
    "#     time_start = time.time()\n",
    "#     fed_freq = torch.tensor(data_nums, dtype=torch.float) / sum(data_nums)\n",
    "#     global_state_dict = average_weights(active_state_dict, fed_freq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--device DEVICE]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--save_every SAVE_EVERY] [--verbose VERBOSE]\n",
      "                             [--seed SEED] [--iid IID] --alg\n",
      "                             {fedavg,fedprox,moon,scaffold,Elastic,Hyper}\n",
      "                             --dataset {smd,smap,psm,swat,skab} --tsadalg\n",
      "                             {gdn,deep_svdd,usad,tran_ad,lstm_ae,transformer,itransformer}\n",
      "                             [--num_clients NUM_CLIENTS]\n",
      "                             [--slide_win SLIDE_WIN]\n",
      "                             [--client_rate CLIENT_RATE] [--beta BETA]\n",
      "                             [--mu MU] [--tau TAU]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --alg, --dataset, --tsadalg\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets.MOON_util import partition_data, get_dataset\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--device DEVICE]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--save_every SAVE_EVERY] [--verbose VERBOSE]\n",
      "                             [--seed SEED] [--iid IID] --alg\n",
      "                             {fedavg,fedprox,moon,scaffold,Elastic,Hyper}\n",
      "                             --dataset {smd,smap,psm,swat,skab} --tsadalg\n",
      "                             {gdn,deep_svdd,usad,tran_ad,lstm_ae,transformer,itransformer}\n",
      "                             [--num_clients NUM_CLIENTS]\n",
      "                             [--slide_win SLIDE_WIN]\n",
      "                             [--client_rate CLIENT_RATE] [--beta BETA]\n",
      "                             [--mu MU] [--tau TAU]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --alg, --dataset, --tsadalg\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "from datasets.MOON_util import partition_data, get_dataset\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
